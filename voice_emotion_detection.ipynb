{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FcgBI7ZTgwhQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as df\n",
        "import sounddevice as sd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tkinter import Tk, Button, Label, filedialog, messagebox\n",
        "import os\n",
        "from glob import glob\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocess_audio(file_path):\n",
        "  y,sr=librosa.load(file_path, sr=None)\n",
        "  y_trimmed,_=librosa.effects.trim(y,top_db=20)\n",
        "  mfcc=librosa.feature.mfcc(y=y_trimmed,sr=sr,n_mfcc=13)\n",
        "  return np.mean(mfcc.T,axis=0)"
      ],
      "metadata": {
        "id": "pTtPTzdrimSK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_emotion_label(file_path):\n",
        "  file_name = os.path.basename(file_path)\n",
        "  emotion_code = int(file_name.split('-')[2])\n",
        "  emotion_map={\n",
        "      1:\"neutral\",2:\"calm\",3:\"happy\",4:\"sad\",\n",
        "      5:\"angry\",6:\"fearful\",7:\"disgust\",\n",
        "      8:\"surprised\"\n",
        "  }\n",
        "  return emotion_map.get(emotion_map).get(emotion_code,\"unknown\")"
      ],
      "metadata": {
        "id": "WTZFE9dNk75p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_gender(file_path):\n",
        "  file_name = os.path.basename(file_path)\n",
        "  gender_code = int(file_name.split('-')[3])\n",
        "  gender_map={1:'male',2:'female'}\n",
        "  return gender_map.get(gender_code,\"unknown\")"
      ],
      "metadata": {
        "id": "fyL0AntMlP47"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_emotion_model(audio_files,labels):\n",
        "  features = [load_preprocess_audio(file) for file in audio_files]\n",
        "  X=np.array(features)\n",
        "  y=np.array(labels)\n",
        "  le=LabelEncoder()\n",
        "  y_encoded=le.fit_transform(y)\n",
        "  X_train,X_test,y_train,y_test=train_test_split(X,y_encoded,test_size=0.2,random_state=42)\n",
        "  model=SVC(kernel='linear')\n",
        "  model.fit(X_train,y_train)\n",
        "  y_pred=model.predict(X_test)\n",
        "  accuracy=accuracy_score(y_test,y_pred)\n",
        "  print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "  return model,le"
      ],
      "metadata": {
        "id": "gRZ0sHUHmHVM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gui"
      ],
      "metadata": {
        "id": "ZBEc0YTynVy4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionDetectionApp:\n",
        "  def __init__(self, root):\n",
        "    self.root=root\n",
        "    self.root.title(\"Emotion Detection through voice\")\n",
        "    self.model=None\n",
        "    self.le=None\n",
        "\n",
        "    Label(root,text=\"Upload Audio File\").pack()\n",
        "    Button(root,text=\"Upload voice note\", command=self.upload_voice_note).pack()\n",
        "    Button(root, text=\"Record Voice\", command=self.record_voice).pack()\n",
        "\n",
        "  def upload_voice_note(self):\n",
        "    file_path = filedialog.askopenfilename(filetypes=[(\"Audio Files\", \"*.wav*.mp3\")])\n",
        "    if file_path:\n",
        "      self.process_audio(file_path)\n",
        "\n",
        "  def record_voice(self):\n",
        "    duration = 5\n",
        "    fs = 44100\n",
        "    messagebox.showinfo(\"Recording\", \"Recording will start now. Please speak into the microphone for 5 seconds.\")\n",
        "    audio_data = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
        "    sd.wait()\n",
        "    file_path(\"recorded_voice.wav\")\n",
        "    sf.write(file_path, audio_data,fs)\n",
        "    self.process_audio(file_path)\n",
        "\n",
        "  def process_audio(self,file_path):\n",
        "    gender = detect_gender(file_path)\n",
        "    if gender != \"female\":\n",
        "      messagebox.showerror(\"Error\", \"Please upload a female voice note.\")\n",
        "      return\n",
        "    features = load_preprocess_audio(file_path)\n",
        "    if self.model:\n",
        "      emotion_encoded = self.model.predict([features])\n",
        "      emotion = self.le.inverse_transform(emotion_encoded)\n",
        "      messagebox.showinfo(\"Emotion Detected\",f\"The Detected emotion is: {emotion[0]}\")\n",
        "    else:\n",
        "      messagebox.showerror(\"Error\", \"Model not trained. Please upload a voice note first.\")\n",
        ""
      ],
      "metadata": {
        "id": "hbifYX2Hnzf9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  dataset_path = \"/kaggle/input/ravdess-emotional-speech-audio/*/*.wav\"\n",
        "  audio_files = glob(dataset_path)\n",
        "  labels = [extract_emotion_label(file) for file in audio_files]\n",
        "  audio_files = [file for file in audio_files if extract_emotion_label(file) != \"unknown\"]\n",
        "  labels = [label for label in labels if label != \"unknown\"]\n",
        "  model,le=train_emotion_model(audio_files,labels)\n",
        "  root=Tk()\n",
        "  app=EmotionDetectionApp(root)\n",
        "  app.model=model\n",
        "  app.le=le\n",
        "  root.mainloop()\n",
        "  labels = [extract_emotion_label(file) for file in audio_files]\n",
        "  audio_files = [file for file in audio_files if extract_emotion_label(file) != \"unknown\"]\n",
        "  labels = [label for label in labels if label != \"unknown\"]\n",
        "  model,le=train_emotion_model(audio_files,labels)\n",
        "  root=Tk()\n",
        "  app=EmotionDetectionApp(root)\n",
        "  app.model=model\n",
        "  app.le=le\n",
        "  root.mainloop()"
      ],
      "metadata": {
        "id": "Aaj3uQ3Ts60y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RVzBtxJrvn0d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}